{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# autoreload to reload modules when they change\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import rollout\n",
    "from torch import optim\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "\n",
    "import networkx  as nx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "\n",
    "def generate_reasoning_combinations(num_tokens, sequence_length, n_back, return_output=True):\n",
    "    # generate all permutations of the first sequence_length-1 elements\n",
    "    # TODO: Could subsample the permutations to reduce the number of combinations\n",
    "    G = nx.cycle_graph(num_tokens)\n",
    "    all_perms = list(permutations(range(num_tokens), sequence_length-1))\n",
    "    combinations = torch.zeros(len(all_perms), sequence_length, dtype=torch.long)\n",
    "    combinations[:,:sequence_length-1] = torch.tensor(all_perms)\n",
    "    # now set the last element to a random element in the sequence\n",
    "    prompt_inds = torch.randint(0, sequence_length-1-n_back, (len(combinations),))\n",
    "    combinations[:,-1] = combinations[np.arange(len(combinations)),prompt_inds]\n",
    "    if return_output:\n",
    "        output_inds = prompt_inds + n_back\n",
    "        output = combinations[np.arange(len(combinations)),output_inds]\n",
    "        reasoned_outputs = (output + 1) % num_tokens\n",
    "        \n",
    "        return combinations, reasoned_outputs\n",
    "    return combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens = 10\n",
    "sequence_length = 6\n",
    "n_back = 1\n",
    "comb, reason = generate_reasoning_combinations(num_tokens=n_tokens, sequence_length=sequence_length, n_back=n_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "class ReasoningDataset:\n",
    "    # A dataset class for generating sequences with induction patterns.\n",
    "    # TODO could subclass torch.utils.data.Dataset for more flexibility\n",
    "    def __init__(self, num_tokens, sequence_length, n_back=1, random_seed=42, train_fraction=0.8, data_generator=generate_reasoning_combinations):\n",
    "        \"\"\"\n",
    "        Initializes the InductionDataset with the given parameters.\n",
    "        Args:\n",
    "            num_tokens (int): Total number of unique tokens.\n",
    "            sequence_length (int): Length of each sequence.\n",
    "            n_back (int, optional): Number of steps back to look for the induction pattern. Defaults to 1.\n",
    "            random_seed (int, optional): Random seed for reproducibility. Defaults to 42.\n",
    "            train_fraction (float, optional): Fraction of data to be used for training. Defaults to 0.8.\n",
    "        \"\"\"\n",
    "        torch.manual_seed(random_seed)\n",
    "        assert num_tokens > sequence_length, \"num_tokens must be greater than sequence_length\"\n",
    "        assert n_back < sequence_length-1, \"n_back must be less than sequence_length-1\"\n",
    "        self.n = num_tokens\n",
    "        self.n_back = n_back\n",
    "\n",
    "        self.X, self.y = data_generator(num_tokens, sequence_length, n_back)\n",
    "        shuffle_idx = torch.randperm(len(self.X))\n",
    "        self.X = self.X[shuffle_idx]\n",
    "        self.y = self.y[shuffle_idx]\n",
    "        self.n_samples = len(self.X)\n",
    "\n",
    "        self.n_train = int(self.n_samples * 0.8)\n",
    "        self.train_idx = torch.arange(self.n_train)\n",
    "\n",
    "        self.test_idx = torch.arange(self.n_train, self.n_samples)\n",
    "        self.n_test = len(self.test_idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def generate_batch(self, batch_size, type='train'):\n",
    "        \"\"\"\n",
    "        Generates a batch of data for training or testing.\n",
    "        Args:\n",
    "            batch_size (int): Number of samples in the batch.\n",
    "            type (str, optional): Type of data to generate ('train' or 'test'). Defaults to 'train'.\n",
    "        Returns:\n",
    "            tuple: A tuple containing the input sequences (X) and the output sequences (y).\n",
    "        \"\"\"\n",
    "        assert type in ['train', 'test'], \"type must be either 'train' or 'test'\"\n",
    "        if type == 'train':\n",
    "            idx = self.train_idx[torch.randint(0, self.n_train, (batch_size,))]\n",
    "        else:\n",
    "            idx = self.test_idx[torch.randint(0, self.n_test, (batch_size,))]\n",
    "        X = self.X[idx]\n",
    "        y = self.y[idx]\n",
    "        return X, y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss: 2.4965081214904785, Test Loss: 2.366917848587036\n",
      "Epoch 100: Train Loss: 1.3366155624389648, Test Loss: 1.344161033630371\n",
      "Epoch 200: Train Loss: 1.315226435661316, Test Loss: 1.3030773401260376\n",
      "Epoch 300: Train Loss: 1.2873501777648926, Test Loss: 1.291556477546692\n",
      "Epoch 400: Train Loss: 1.3003287315368652, Test Loss: 1.2621301412582397\n"
     ]
    }
   ],
   "source": [
    "d_model = 256\n",
    "n_tokens = 10\n",
    "sequence_length = 6\n",
    "n_heads = 1\n",
    "dataset = ReasoningDataset(n_tokens, sequence_length)\n",
    "simpleModel = rollout.models.FlexibleTransformer(d_model, n_tokens, sequence_length, n_heads, n_attn_layers=1)\n",
    "optimizer = optim.AdamW(simpleModel.parameters(), lr=0.001)\n",
    "criterion = nn.functional.cross_entropy\n",
    "\n",
    "simple_train_losses, simple_test_losses = rollout.models.optimize_model(simpleModel, criterion, optimizer, dataset, n_epochs=500, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss: 2.528334140777588, Test Loss: 2.7821390628814697\n",
      "Epoch 100: Train Loss: 0.00027958714053966105, Test Loss: 0.00026847951812669635\n",
      "Epoch 200: Train Loss: 8.501573756802827e-05, Test Loss: 8.46980547066778e-05\n",
      "Epoch 300: Train Loss: 5.293273716233671e-05, Test Loss: 5.359837814467028e-05\n",
      "Epoch 400: Train Loss: 3.7338297261158004e-05, Test Loss: 3.9357313653454185e-05\n"
     ]
    }
   ],
   "source": [
    "d_model = 256\n",
    "n_tokens = 10\n",
    "sequence_length = 6\n",
    "n_heads = 1\n",
    "dataset = ReasoningDataset(n_tokens, sequence_length)\n",
    "complexModel = rollout.models.FlexibleTransformer(d_model, n_tokens, sequence_length, n_heads, n_attn_layers=2)\n",
    "optimizer = optim.AdamW(complexModel.parameters(), lr=0.001)\n",
    "criterion = nn.functional.cross_entropy\n",
    "\n",
    "complex_train_losses, complex_test_losses = rollout.models.optimize_model(complexModel, criterion, optimizer, dataset, n_epochs=500, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TransformerCircuits",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
